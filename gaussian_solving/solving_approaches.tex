\section{Different approaches to solving parameters}
Since our method enables us to propose different solutions for each parameter (in cases of overdetermined systems), we can propose few means of choosing the final value for given parameter.
These can rely on choosing the best solution out of all that are possible, or combining several best choices together.
\begin{enumerate}
    \item \textbf{Choose the best candidate for a solution out of $k$ best guesses.}

        We have to remember that in cases of ambiguity for each parameter, there are infinitely many linear combinations of first solution and the zero vectors.
        If we would like to pick the best solution, we have to restrict our search space to the subset of $k$ best candidates.
        In order to compare candidates for a solution, we can propose a fitness function that tries to estimate the error associated with each solution.
        %Ideally, our method would yield the best solution as the initial first solution in reduced row echelon form, which can be achieved if we include the idea behind given fitness function in Gauss-Jordan elimination.
    \item \textbf{Combine $k$ possible candidates for solution into a final answer}
        
        Once we elicit the set of $k$ possible candidates, we can use weighted average to combine them into final result.
        This will not automatically improve the result, but will average-out the error, which in most cases may result good solutions.
        This may be especially useful in case where it's difficult to propose a fitness function that describes the error accurately.
\end{enumerate}
Since both approaches rely on similar input (fitness function, initial set of $k$ candidates) we'll try to describe it first.

\begin{description}
    \item[Candidates for the solution]

    As we had shown previously, outcome of Gauss--Jordan elimination is just one of the possible solutions in overdetermined systems of equations.
\end{description}

\begin{description}
    \item[Candidates for fitness function] \hfill \\
    \begin{enumerate}
        \item \textbf{Relative frequency of each combination in a data file}
        Intuitively it is clear that the error is inversely proportional to the frequency of given combination within the data file.
        First good guess for a fitness function could be the frequency with which given equation appears in a data file.
    \end{enumerate}
\end{description}

\subsection{Examples}
Let us append some quality measures to each equation in the dataset.
The whole equation set adds up to 2500 records.
We can calculate a frequency for every vector, and treat it as a weight.

\begin{equation}
\begin{matrix}[cccc|c|c|c]
     &  &  &  & \mbox{probability} & \mbox{quantity} & \mbox{frequency Fq($b_i$)} \\
    \hline
    0 & 0 & 1 & 0 & b_1 & 980 & 0.392 \\
    1 & 1 & 1 & 0 & b_2 & 760 & 0.304 \\ 
    1 & 1 & 0 & 0 & b_3 & 440 & 0.176 \\ 
    0 & 0 & 0 & 1 & b_4 & 320 & 0.128 \\ 
    \hline
     &  &  &  &  & 2500 & \\ 
\end{matrix}
\end{equation}

Using this data we can propose few heuristics for calculating final value of $x_3$ or compare different solutions.
Let us propose a fitness function for a solution:

\begin{equation}
    F(s) = \frac{\displaystyle\prod_{b_{i} \in s} Fq(b_i)}{ \displaystyle\sum_{b_i \in s}1},
\end{equation},
where $b_i \in s$ is true when $b_i$ is taken into account (adding or subtracting) in given solution.

\begin{equation}
\begin{matrix}[cccc|c|c]
     &  &  &  & \mbox{probability} & \mbox{fitness function F(s)} \\
    \hline
    [0 & 0 & 1 & 0]_1 & b_1 & 0.392 \\
    [0 & 0 & 1 & 0]_2 & b_2 - b_3 & \frac{0.304 \cdot 0.176}{2} = 0.24 \\ 
\end{matrix}
\end{equation}

At this point we can pick a solution with a higher fitness value, or take weighted average of each solution as our final answer:

\begin{equation}
\begin{matrix}[cccc|c]
     &  &  &  & \mbox{probability}\\
    \hline
    [0 & 0 & 1 & 0] & \frac{0.392 \cdot b_1 + 0.24 \cdot (b_2 - b_3)}{0.392+0.24}\\
\end{matrix}
\end{equation}


